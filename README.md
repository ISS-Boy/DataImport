# DataImport
> This project aims to import data generated by mhealth and synthea to kafka,
> better abstraction needed to refactor some implement details.
## 数据格式

### 动态数据
> 数据频率较高，低于一天/次，由MHealth生成

| Measures | 生成值区间            | 数据频率    |          备注          |
| -------- | ---------------- | ------- | :------------------: |
| 舒张压      | 60-90mmhg        | 每分钟含夜晚  | 每分钟生成一条血压数据值在60-90之间 |
| 收缩压      | 90-140mmhg       | 每分钟含夜晚  |          -           |
| 心率       | 60-120次/m        | 每分钟含夜晚  |          -           |
| 步数       | default          | 每小时不含夜晚 |  default为配置文件当前默认值   |
| 体温       | default(36.4-38) | 每小时含夜晚  |          -           |
| 睡眠时间     | default          | 每天      |          -           |
| ……       |                  |         |    具体频率及种类见项目配置文件    |


- 原始数据为json格式

  ```json
  //格式描述
  {
      "header": {
          "id": "423c4b46-15ac-438b-9734-f8556cb94b6a",
          "creation_date_time": "2015-01-01T15:34:25Z",
          "acquisition_provenance": {
              "source_name": "generator",
              "source_creation_date_time": "2015-01-01T15:33:25Z",
              "modality": "sensed"
          },
          "user_id": "some-user",
          "schema_id": {
              "namespace": "omh",
              "name": "body-weight",
              "version": "1.0"
          }
      },
      "body": {
          "effective_time_frame": {
              "date_time": "2015-01-01T15:33:25Z"
          },
          "body_weight": {
              "unit": "kg",
              "value": 60.01255983207784
          }
      },
      "id": "423c4b46-15ac-438b-9734-f8556cb94b6a"
  }
  ```


### 静态数据

- 半静态数据格式

  > 数据频率较低，高于一周/次，由Synthea生成, csv文件格式

  FHIR生命体征数据![FHIR](http://ovr73w1dl.bkt.clouddn.com/FHIR.png)

  当前问题：

  - 如何修改实现，使其按照配置的频率生成数据。
  - 如何解析数据，过滤其他信息，只获得生命体征数据。

- 静态数据格式

  > 模拟后永远不发生改变，可按照统计概率自己生成

  性别，遗传等

### 经纬度数据
> 新增的经纬度数据,用于web端展示 

## 实现细节
- FileSystem
    - 数据生成后保存在文件系统中(分别有mhealth,synthea,经纬度数据),
我们使用jdk提供的文件操作类RandomAccessFile来做数据导入,其使用操作系统的mmap()来做内存映射,
可以直接通过offset读取大文件的对应偏移量处的数据,
同时为了多线程读入,我们将用户分组,每组为20个用户,共10000个用户,并通过offset的控制来实现数据的复用.

-  Concurrency control
    - 闭锁CountDownLatch 控制线程同时读取
    - Object.wait(),notify()原语做数据复用
    - 队列消费者(kafka producer)线程池

- Avro
    - 数据以avro格式写入kafka中,一方面是考虑序列化以后的数据size,另一方面是为了给数据提供结构(视图)